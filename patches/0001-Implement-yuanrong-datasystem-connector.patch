From 4edb11357ea301312456d7cc8ae4a7fddf1d020b Mon Sep 17 00:00:00 2001
From: yangsonglin13 <yangsonglin566@gmail.com>
Date: Thu, 20 Nov 2025 17:09:53 +0800
Subject: [PATCH] Implement yuanrong-datasystem connector.

---
 .github/workflows/vllm_ascend_test_pd.yaml    |    5 +
 tests/e2e/pd_disaggreate/yuanrong/README.md   |  155 ++
 .../yuanrong/benchmark/calculate_hit_rates.py |   65 +
 .../yuanrong/benchmark/gen_random_dataset.py  |  130 ++
 .../yuanrong/benchmark/run_benchmark.sh       |   27 +
 .../benchmark/run_pd_merge_instances.sh       |   46 +
 .../pd_disaggreate/yuanrong/clean_yuanrong.sh |    9 +
 .../yuanrong/run_pd_instances.sh              |   56 +
 .../yuanrong/run_proxy_server.sh              |   14 +
 .../pd_disaggreate/yuanrong/run_yuanrong.sh   |   24 +
 .../yuanrong/simple_pd_proxy_server.py        |  212 +++
 .../yuanrong/test_yuanrong_connector.py       |  141 ++
 vllm_ascend/distributed/__init__.py           |    5 +
 vllm_ascend/distributed/yuanrong_connector.py | 1326 +++++++++++++++++
 14 files changed, 2215 insertions(+)
 create mode 100644 tests/e2e/pd_disaggreate/yuanrong/README.md
 create mode 100644 tests/e2e/pd_disaggreate/yuanrong/benchmark/calculate_hit_rates.py
 create mode 100644 tests/e2e/pd_disaggreate/yuanrong/benchmark/gen_random_dataset.py
 create mode 100644 tests/e2e/pd_disaggreate/yuanrong/benchmark/run_benchmark.sh
 create mode 100644 tests/e2e/pd_disaggreate/yuanrong/benchmark/run_pd_merge_instances.sh
 create mode 100644 tests/e2e/pd_disaggreate/yuanrong/clean_yuanrong.sh
 create mode 100644 tests/e2e/pd_disaggreate/yuanrong/run_pd_instances.sh
 create mode 100644 tests/e2e/pd_disaggreate/yuanrong/run_proxy_server.sh
 create mode 100644 tests/e2e/pd_disaggreate/yuanrong/run_yuanrong.sh
 create mode 100644 tests/e2e/pd_disaggreate/yuanrong/simple_pd_proxy_server.py
 create mode 100644 tests/e2e/pd_disaggreate/yuanrong/test_yuanrong_connector.py
 create mode 100644 vllm_ascend/distributed/yuanrong_connector.py

diff --git a/.github/workflows/vllm_ascend_test_pd.yaml b/.github/workflows/vllm_ascend_test_pd.yaml
index b031e08..3d18dfc 100644
--- a/.github/workflows/vllm_ascend_test_pd.yaml
+++ b/.github/workflows/vllm_ascend_test_pd.yaml
@@ -110,3 +110,8 @@ jobs:
         run: |
           git config --global --add safe.directory/__w/vllm-ascend/vllm-ascend
           bash tests/e2e/pd_disaggreate/run_edge_case_test.sh
+
+
+      - name: Run vllm-project/vllm-ascend PD Disaggregation test with YuanRong Connector
+        run: |
+          pytest -sv tests/e2e/pd_disaggreate/yuanrong/test_yuanrong_connector.py
diff --git a/tests/e2e/pd_disaggreate/yuanrong/README.md b/tests/e2e/pd_disaggreate/yuanrong/README.md
new file mode 100644
index 0000000..b19f0fa
--- /dev/null
+++ b/tests/e2e/pd_disaggreate/yuanrong/README.md
@@ -0,0 +1,155 @@
+# Overview: transfer KVCache through host memory with YuanRongConnector
+
+### Dataflow
+
+
+|----------------------- prefill node ---------------------| &nbsp; |---------------------- decode node ---------------------|
+
+Prefill Instance  -----> YuanRongConnector -----> YuanRong Data Worker -----> YuanRongConnector -----> Decode Instance
+
+|----- kv on npu -----| &nbsp; |----- kv offload to host -----| &nbsp; |----- kv transfer by host net -----| &nbsp; |----- kv load to npu -----|
+
+### Pros
+- Network jitter and failures are handled outside of the vLLM process, better isolation and fault tolerance
+- No need to allocate communication buffers on NPU, enable a larger sequence batch and throughput
+- Work seamlessly with features those require offloading kvcache to host memory or SSD, like prefix cache, priority scheduling, RAG, etc.
+### Cons
+- Higher transfer latency compared with device-to-device transfer, not optimal for latency-sensitive scenarios
+
+
+
+
+
+# Installation
+
+## Install etcd
+#### 1. Download the latest binaries from [etcd github releases](https://github.com/etcd-io/etcd/releases)
+```
+ETCD_VERSION="v3.5.12"  
+wget https://github.com/etcd-io/etcd/releases/download/${ETCD_VERSION}/etcd-${ETCD_VERSION}-linux-amd64.tar.gz
+```
+#### 2. Unzip and install
+```
+tar -xvf etcd-${ETCD_VERSION}-linux-amd64.tar.gz
+cd etcd-${ETCD_VERSION}-linux-amd64
+# copy the binary to system
+sudo cp etcd etcdctl /usr/local/bin/
+```
+#### 3. Verify installation
+```
+etcd --version
+etcdctl version
+```
+
+
+## Install openyuanrong-datasystem
+#### Install from pip (recommended):
+
+```
+pip install openyuanrong-datasystem
+```
+
+#### Or install from source:
+
+- Refer to the openyuanrong-datasystem documentation [here](https://gitee.com/openeuler/yuanrong-datasystem)
+
+
+
+# Deployment
+## Deploy etcd
+> Note: this is the minimal example to deploy etcd, more can be found at the [etcd official site](https://etcd.io/docs/current/op-guide/clustering/).
+
+#### Deploy a single node etcd cluster at port 2379:
+```
+etcd \
+  --name etcd-single \
+  --data-dir /tmp/etcd-data \
+  --listen-client-urls http://0.0.0.0:2379 \
+  --advertise-client-urls http://0.0.0.0:2379 \
+  --listen-peer-urls http://0.0.0.0:2380 \
+  --initial-advertise-peer-urls http://0.0.0.0:2380 \
+  --initial-cluster etcd-single=http://0.0.0.0:2380
+```
+
+
+#### Parameters:
+- --name：cluster name
+- --data-dir：directory to store data
+- --listen-client-urls：address to listen from clients (0.0.0.0 allows access from any IP address)
+- --advertise-client-urls：address advertised to clients
+- --listen-peer-urls：address to listen from other nodes in the cluster
+- --initial-advertise-peer-urls：address advertised to other nodes in the cluster
+- --initial-cluster：initial nodes in the cluster (format: name1=peer_url1,name2=peer_url2,...)
+
+#### Try to access the etcd cluster with the `etcdctl` command:
+```
+etcdctl --endpoints "127.0.0.1:2379" put key "value"
+etcdctl --endpoints "127.0.0.1:2379" get key
+```
+etcd cluster is successfully deployed if the commands work good.
+
+## Deploy openyuanrong-datasystem
+#### Deploy a single node openyuanrong-datasystem cluster with the minimum config:
+```
+dscli start -w --worker_address "127.0.0.1:31501" --etcd_address "127.0.0.1:2379"
+# [INFO] [  OK  ] Start worker service @ 127.0.0.1:31501 success, PID: 38100
+```
+openyuanrong-datasystem is deployed successful as you see the `[  OK  ]` output.
+
+#### To safely stop and clean the openyuanrong-datasystem processes, run the command:
+```
+dscli stop -w --worker_address "127.0.0.1:31501"
+```
+#### Please refer to the [openyuanrong-datasystem gitee repo](https://gitee.com/openeuler/yuanrong-datasystem) for more information.
+
+# Run disaggregated prefill with vLLM v1
+
+> Note: an example script for 1P1D disaggregated prefill is available at: *vllm-ascend/tests/e2e/pd_disaggregate/yuanrong/test_yuanrong_connector.py*
+
+#### 1. Populate the openyuanrong-datasystem worker address with environment variable:
+
+`export DS_WORKER_ADDR=127.0.0.1:31501`
+
+YuanRongConnector will read the openyuanrong-datasystem address from this environment variable
+
+#### 2. Start two vLLM instances with YuanRongConnector as the backend to form a 1P1D disaggregated cluster:
+```
+export VLLM_USE_V1=True
+
+# start a prefill instance on localhost:8100
+ASCEND_RT_VISIBLE_DEVICES=0 vllm serve Qwen/Qwen2.5-7B-Instruct \
+    --port 8100 \
+    --max-num-batched-tokens 45000 \
+    --gpu-memory-utilization 0.8 \
+    --trust-remote-code \
+    --enforce-eager \
+    --kv-transfer-config \
+    '{"kv_connector":"YuanRongConnector","kv_role":"kv_producer","kv_rank":0,"kv_parallel_size":2}' &
+
+# start a decode instance on localhost:8200
+ASCEND_RT_VISIBLE_DEVICES=1 vllm serve Qwen/Qwen2.5-7B-Instruct \
+    --port 8200 \
+    --max-num-batched-tokens 45000 \
+    --gpu-memory-utilization 0.8 \
+    --trust-remote-code \
+    --enforce-eager \
+    --kv-transfer-config \
+    '{"kv_connector":"YuanRongConnector","kv_role":"kv_consumer","kv_rank":1,"kv_parallel_size":2}' &
+```
+
+#### 3. Start a proxy server to serve and route HTTP requests:
+```
+python vllm-ascend/tests/e2e/pd_disaggregate/yuanrong/simple_pd_proxy_server.py --prefiller-port 8100 --decoder-port 8200
+```
+
+#### 4. Send HTTP requests to the proxy server:
+```
+curl -X POST -s http://localhost:8000/v1/completions \
+-H "Content-Type: application/json" \
+-d '{
+"model": "Qwen/Qwen2.5-7B-Instruct",
+"prompt": "who is the presiden of the united states?",
+"max_tokens": 50,
+"temperature": 0
+}'
+```
diff --git a/tests/e2e/pd_disaggreate/yuanrong/benchmark/calculate_hit_rates.py b/tests/e2e/pd_disaggreate/yuanrong/benchmark/calculate_hit_rates.py
new file mode 100644
index 0000000..ab444fd
--- /dev/null
+++ b/tests/e2e/pd_disaggreate/yuanrong/benchmark/calculate_hit_rates.py
@@ -0,0 +1,65 @@
+import re
+import sys
+
+def calculate_hit_rates(file_path):
+    # Initialize statistical variables
+    sum_total_tokens = 0
+    sum_hbm_hit = 0
+    sum_ddr_hit = 0
+    line_count = 0
+
+    # Regular expression to match numbers
+    # Matching format: Total tokens 8212, HBM hit tokens: 4096, External hit tokens: 0
+    pattern = re.compile(r"Total tokens\s+(\d+),.*?HBM hit tokens:\s+(\d+),.*?External hit tokens:\s+(\d+)")
+
+    try:
+        with open(file_path, 'r', encoding='utf-8') as f:
+            for line in f:
+                # Simple filtering, only process lines containing specific keywords
+                if "Total tokens" not in line:
+                    continue
+
+                match = pattern.search(line)
+                if match:
+                    total = int(match.group(1))
+                    hbm = int(match.group(2))
+                    load = int(match.group(3))
+
+                    sum_total_tokens += total
+                    sum_hbm_hit += hbm
+                    sum_ddr_hit += load
+                    line_count += 1
+
+        if sum_total_tokens == 0:
+            print("No valid Token data found.")
+            return
+
+        # Calculate hit rates
+        # Note: Due to Block alignment in vLLM, hit + load may be slightly larger than total, calculated by actual values here
+        hbm_rate = (sum_hbm_hit / sum_total_tokens) * 100
+        ddr_rate = (sum_ddr_hit / sum_total_tokens) * 100
+        total_hit_rate = ((sum_hbm_hit + sum_ddr_hit) / sum_total_tokens) * 100
+
+        print("-" * 40)
+        print(f"Log Analysis Result ({file_path})")
+        print("-" * 40)
+        print(f"Processed Lines (Requests) : {line_count}")
+        print(f"Total Token Count          : {sum_total_tokens}")
+        print(f"HBM Hit (GPU) Count        : {sum_hbm_hit}")
+        print(f"DDR Hit (CPU) Count        : {sum_ddr_hit}")
+        print("-" * 40)
+        print(f"HBM Hit Rate (GPU)         : {hbm_rate:.2f}%")
+        print(f"DDR Hit Rate (CPU)         : {ddr_rate:.2f}%")
+        print(f"Overall Hit Rate           : {total_hit_rate:.2f}%")
+        print("-" * 40)
+
+    except FileNotFoundError:
+        print(f"Error: File {file_path} not found")
+    except Exception as e:
+        print(f"Error occurred: {e}")
+
+if __name__ == "__main__":
+    if len(sys.argv) < 2:
+        print("Usage: python calc_hit_rate.py <log_file_name>")
+    else:
+        calculate_hit_rates(sys.argv[1])
diff --git a/tests/e2e/pd_disaggreate/yuanrong/benchmark/gen_random_dataset.py b/tests/e2e/pd_disaggreate/yuanrong/benchmark/gen_random_dataset.py
new file mode 100644
index 0000000..cc8ab70
--- /dev/null
+++ b/tests/e2e/pd_disaggreate/yuanrong/benchmark/gen_random_dataset.py
@@ -0,0 +1,130 @@
+import json
+import logging
+import os
+import random
+import string
+from tqdm import tqdm
+from transformers import AutoTokenizer
+
+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+
+
+def gen_random_string(length=10):
+    """Generate a random string of specified length"""
+    return "".join(random.choices(string.ascii_letters + string.digits, k=length))
+
+
+def gen_random_tokens(tokenizer, num_tokens):
+    """
+    Generate a piece of random text that, after tokenization, has exactly the specified number of tokens.
+
+    Args:
+        tokenizer: The transformers tokenizer instance to use.
+        num_tokens: Target number of tokens.
+
+    Returns:
+        A randomly generated text string.
+    """
+    token_ids = []
+    while len(token_ids) < num_tokens:
+        random_word = gen_random_string(random.randint(3, 8))
+        token_ids.extend(tokenizer.encode(" " + random_word, add_special_tokens=False))
+
+    final_token_ids = token_ids[:num_tokens]
+    return tokenizer.decode(final_token_ids)
+
+
+def gen_random_prompts(tokenizer, num_groups, num_prompts_per_group, prefix_tokens, suffix_tokens):
+    """
+    Generate a random prompt dataset with length based on token count.
+
+    Args:
+        tokenizer: The transformers tokenizer instance to use.
+        num_groups: Number of groups.
+        num_prompts_per_group: Number of prompts per group.
+        prefix_tokens: Number of prefix tokens.
+        suffix_tokens: Number of suffix tokens.
+
+    Returns:
+        A list of randomly generated prompts.
+    """
+    prompts = []
+    logging.info(
+        f"Starting to generate dataset (Number of groups: {num_groups}, Prompts per group: {num_prompts_per_group})...")
+
+    for _ in tqdm(range(num_groups), desc="Generating groups"):
+        prefix = gen_random_tokens(tokenizer, prefix_tokens)
+        for _ in range(num_prompts_per_group):
+            suffix = gen_random_tokens(tokenizer, suffix_tokens)
+            prompt = prefix + " " + suffix
+            prompts.append(prompt)
+
+    random.shuffle(prompts)
+    return prompts
+
+
+def save_to_file(prompts, output_file):
+    """Save generated prompts to a JSONL file"""
+    with open(output_file, 'w', encoding='utf-8') as f:
+        for prompt in tqdm(prompts, desc="Writing to file"):
+            data = {"prompt": prompt}
+            json_line = json.dumps(data, ensure_ascii=False)
+            f.write(json_line + '\n')
+
+    logging.info(f"Successfully saved {len(prompts)} entries to {output_file}")
+
+
+def main():
+    # ==============================================================================
+    # Configuration - Parts you need to modify
+    # ==============================================================================
+    config = {
+        # Fill in your local model folder path or model name on Hugging Face here
+        # For example: 'gpt2', './my_local_llama_model', 'Qwen/Qwen1.5-7B-Chat'
+        'tokenizer_path': '/data/models/qwen2.5_7B_Instruct',
+        'num_groups': 30,
+        'num_prompts_per_group': 100,
+        'prefix_length': 12 * 1024,  # Prefix token count
+        'suffix_length': 12 * 1024,  # Suffix token count
+        'output_dir': '.',
+        'output_file': 'dataset_24k_tokens_50p.jsonl',
+        'seed': 42
+    }
+
+    # Set random seed
+    random.seed(config['seed'])
+
+    # ==============================================================================
+    # Load generic tokenizer from specified path
+    # ==============================================================================
+    tokenizer_path = config['tokenizer_path']
+    logging.info(f"Loading tokenizer from '{tokenizer_path}'...")
+    try:
+        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
+        if tokenizer.pad_token is None:
+            tokenizer.pad_token = tokenizer.eos_token
+            logging.info("Tokenizer pad_token not set, has been set to eos_token.")
+
+        logging.info("Tokenizer loaded successfully.")
+    except Exception as e:
+        logging.error(f"Error: Unable to load tokenizer from path '{tokenizer_path}'.")
+        logging.error(
+            f"Please confirm if the path is correct and if the folder contains files like " +
+            "'tokenizer.json' or 'tokenizer_config.json'."
+        )
+        logging.error(f"Detailed error message: {e}")
+        return
+
+    os.makedirs(config['output_dir'], exist_ok=True)
+    output_path = os.path.join(config['output_dir'], config['output_file'])
+    total_prompts = config['num_groups'] * config['num_prompts_per_group']
+    total_tokens = config['prefix_length'] + config['suffix_length']
+    logging.info(f"Will generate a total of {total_prompts} entries, each with approximately {total_tokens} tokens.")
+    logging.info(f"(Number of groups: {config['num_groups']}, Prompts per group: {config['num_prompts_per_group']})")
+    prompts = gen_random_prompts(tokenizer, config['num_groups'], config['num_prompts_per_group'],
+                                 config['prefix_length'], config['suffix_length'])
+    save_to_file(prompts, output_path)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/tests/e2e/pd_disaggreate/yuanrong/benchmark/run_benchmark.sh b/tests/e2e/pd_disaggreate/yuanrong/benchmark/run_benchmark.sh
new file mode 100644
index 0000000..5c611b0
--- /dev/null
+++ b/tests/e2e/pd_disaggreate/yuanrong/benchmark/run_benchmark.sh
@@ -0,0 +1,27 @@
+#!/bin/bash
+
+SCRIPT_DIR=$(cd -- "$(dirname "$0")" && pwd)
+LOG_DIR=${LOG_DIR:-"${SCRIPT_DIR}/bench_logs"}
+mkdir -p "${LOG_DIR}"
+
+HOST="127.0.0.1"
+PORT="18544"
+MODEL_PATH="/data/models/qwen2.5_7B_Instruct"
+DATASET_NAME="custom"
+DATASET_PATH="/workspace/benchmark/dataset_8k_tokens_50p.jsonl"
+MAX_CONCURRENCY=8
+RANDOM_OUTPUT_LEN=2
+NUM_PROMPTS=3000
+
+export TORCH_DEVICE_BACKEND_AUTOLOAD=0
+vllm bench serve \
+    --host "$HOST" \
+    --port "$PORT" \
+    --model "$MODEL_PATH" \
+    --dataset-name "$DATASET_NAME" \
+    --dataset-path "$DATASET_PATH" \
+    --max-concurrency "$MAX_CONCURRENCY" \
+    --random-output-len "$RANDOM_OUTPUT_LEN" \
+    --num-prompts "$NUM_PROMPTS" \
+    --save-result \
+    > "${LOG_DIR}/bench.log" 2>&1 &
diff --git a/tests/e2e/pd_disaggreate/yuanrong/benchmark/run_pd_merge_instances.sh b/tests/e2e/pd_disaggreate/yuanrong/benchmark/run_pd_merge_instances.sh
new file mode 100644
index 0000000..f146c3b
--- /dev/null
+++ b/tests/e2e/pd_disaggreate/yuanrong/benchmark/run_pd_merge_instances.sh
@@ -0,0 +1,46 @@
+#!/bin/bash
+
+MODEL_NAME=$1
+HOST_IP=$2
+PORT=$3
+
+SCRIPT_DIR=$(cd -- "$(dirname "$0")" && pwd)
+LOG_DIR=${LOG_DIR:-"${SCRIPT_DIR}/vllm_logs"}
+mkdir -p "${LOG_DIR}"
+
+if python -c "import datasystem" &> /dev/null; then
+    echo "openyuanrong-datasystem is already installed"
+else
+    echo "Install openyuanrong-datasystem ..."
+    python -m pip install openyuanrong-datasystem
+fi
+
+wait_for_server() {
+    local port=$1
+    timeout 1200 bash -c "
+        until curl -s ${HOST_IP}:${port}/v1/completions > /dev/null; do
+            sleep 1
+        done" && return 0 || return 1
+}
+
+ASCEND_RT_VISIBLE_DEVICES=0 vllm serve $MODEL_NAME \
+    --host ${HOST_IP} \
+    --port ${PORT} \
+    --gpu-memory-utilization 0.8 \
+    --max-num-seqs 400 \
+    --max-num-batched-tokens 40000 \
+    --max-model-len 30000 \
+    --seed 1024 \
+    --trust-remote-code \
+    --enforce-eager \
+    --additional-config '{
+        "torchair_graph_config":{"enabled":false},
+        "ascend_scheduler_config":{"enabled":true,"enable_chunked_prefill":true}
+    }' \
+    --kv-transfer-config '{
+        "kv_connector":"YuanRongConnector",
+        "kv_role":"kv_both"
+    }' \
+    > "${LOG_DIR}/pd.log" 2>&1 &
+
+wait_for_server ${PORT}
diff --git a/tests/e2e/pd_disaggreate/yuanrong/clean_yuanrong.sh b/tests/e2e/pd_disaggreate/yuanrong/clean_yuanrong.sh
new file mode 100644
index 0000000..825f7f3
--- /dev/null
+++ b/tests/e2e/pd_disaggreate/yuanrong/clean_yuanrong.sh
@@ -0,0 +1,9 @@
+#!/bin/bash
+
+HOST_IP=$1
+WORKER_PORT=$2
+
+dscli stop \
+    --worker_address ${HOST_IP}:${WORKER_PORT}
+
+pkill etcd
diff --git a/tests/e2e/pd_disaggreate/yuanrong/run_pd_instances.sh b/tests/e2e/pd_disaggreate/yuanrong/run_pd_instances.sh
new file mode 100644
index 0000000..6bd7b54
--- /dev/null
+++ b/tests/e2e/pd_disaggreate/yuanrong/run_pd_instances.sh
@@ -0,0 +1,56 @@
+#!/bin/bash
+
+MODEL_NAME=$1
+HOST_IP=$2
+PREFILL_PORT=$3
+DECODE_PORT=$4
+
+SCRIPT_DIR=$(cd -- "$(dirname "$0")" && pwd)
+LOG_DIR=${LOG_DIR:-"${SCRIPT_DIR}/vllm_logs"}
+mkdir -p "${LOG_DIR}"
+
+if python -c "import datasystem" &> /dev/null; then
+    echo "openyuanrong-datasystem is already installed"
+else
+    echo "Install openyuanrong-datasystem ..."
+    python -m pip install openyuanrong-datasystem
+fi
+
+wait_for_server() {
+    local port=$1
+    timeout 1200 bash -c "
+        until curl -s ${HOST_IP}:${port}/v1/completions > /dev/null; do
+            sleep 1
+        done" && return 0 || return 1
+}
+
+# Start prefill server and redirect logs
+ASCEND_RT_VISIBLE_DEVICES=1 vllm serve $MODEL_NAME \
+    --host ${HOST_IP} \
+    --port ${PREFILL_PORT} \
+    --gpu-memory-utilization 0.9 \
+    --max-num-seqs 128 \
+    --max-num-batched-tokens 10000 \
+    --max-model-len 10000 \
+    --trust-remote-code \
+    --enforce-eager \
+    --kv-transfer-config \
+    '{"kv_connector":"YuanRongConnector","kv_role":"kv_producer","kv_rank":0,"kv_parallel_size":2}' \
+    > "${LOG_DIR}/prefill.log" 2>&1 &
+
+# Start decode server and redirect logs
+ASCEND_RT_VISIBLE_DEVICES=2 vllm serve $MODEL_NAME \
+    --host ${HOST_IP} \
+    --port ${DECODE_PORT} \
+    --gpu-memory-utilization 0.9 \
+    --max-num-seqs 128 \
+    --max-num-batched-tokens 10000 \
+    --max-model-len 10000 \
+    --trust-remote-code \
+    --enforce-eager \
+    --kv-transfer-config \
+    '{"kv_connector":"YuanRongConnector","kv_role":"kv_consumer","kv_rank":1,"kv_parallel_size":2}' \
+    > "${LOG_DIR}/decode.log" 2>&1 &
+
+wait_for_server ${PREFILL_PORT}
+wait_for_server ${DECODE_PORT}
diff --git a/tests/e2e/pd_disaggreate/yuanrong/run_proxy_server.sh b/tests/e2e/pd_disaggreate/yuanrong/run_proxy_server.sh
new file mode 100644
index 0000000..5f1a887
--- /dev/null
+++ b/tests/e2e/pd_disaggreate/yuanrong/run_proxy_server.sh
@@ -0,0 +1,14 @@
+#!/bin/bash
+PROXY_SERVER_SCRIPT=$1
+HOST=$2
+PORT=$3
+PREFILL_PORT=$4
+DECODE_PORT=$5
+
+python ${PROXY_SERVER_SCRIPT} \
+    --host ${HOST} \
+    --port ${PORT} \
+    --prefiller-host ${HOST} \
+    --prefiller-port ${PREFILL_PORT} \
+    --decoder-host ${HOST} \
+    --decoder-port ${DECODE_PORT} &
diff --git a/tests/e2e/pd_disaggreate/yuanrong/run_yuanrong.sh b/tests/e2e/pd_disaggreate/yuanrong/run_yuanrong.sh
new file mode 100644
index 0000000..8b78c23
--- /dev/null
+++ b/tests/e2e/pd_disaggreate/yuanrong/run_yuanrong.sh
@@ -0,0 +1,24 @@
+#!/bin/bash
+
+HOST_IP=$1
+WORKER_PORT=$2
+ETCD_PORT=$3
+
+MASTER_PORT=`expr ${WORKER_PORT} + 1`
+ETCD_PEER_PORT=`expr ${ETCD_PORT} + 1`
+
+etcd \
+    --name etcd-yuanrong \
+    --data-dir /tmp/etcd-yuanrong \
+    --listen-client-urls http://${HOST_IP}:${ETCD_PORT} \
+    --advertise-client-urls http://${HOST_IP}:${ETCD_PORT} \
+    --listen-peer-urls http://${HOST_IP}:${ETCD_PEER_PORT} \
+    --initial-advertise-peer-urls http://${HOST_IP}:${ETCD_PEER_PORT} \
+    --initial-cluster etcd-yuanrong=http://${HOST_IP}:${ETCD_PEER_PORT} &
+
+dscli start \
+    -w \
+    --worker_address ${HOST_IP}:${WORKER_PORT} \
+    --master_address ${HOST_IP}:${MASTER_PORT} \
+    --etcd_address ${HOST_IP}:${ETCD_PORT} \
+    --shared_memory_size_mb 1000000 &
diff --git a/tests/e2e/pd_disaggreate/yuanrong/simple_pd_proxy_server.py b/tests/e2e/pd_disaggreate/yuanrong/simple_pd_proxy_server.py
new file mode 100644
index 0000000..c6b957c
--- /dev/null
+++ b/tests/e2e/pd_disaggreate/yuanrong/simple_pd_proxy_server.py
@@ -0,0 +1,212 @@
+import argparse
+import os
+import time
+from contextlib import asynccontextmanager
+from uuid import uuid4
+
+import httpx
+import numpy as np
+from fastapi import FastAPI, Request
+from fastapi.responses import StreamingResponse
+
+
+@asynccontextmanager
+async def lifespan(app: FastAPI):
+    """
+    Lifespan context manager to handle startup and shutdown events.
+    """
+    # Startup: Initialize clients
+    prefiller_base_url = (
+        f"http://{global_args.prefiller_host}:{global_args.prefiller_port}/v1")
+    decoder_base_url = (
+        f"http://{global_args.decoder_host}:{global_args.decoder_port}/v1")
+
+    app.state.prefill_client = httpx.AsyncClient(timeout=None,
+                                                 base_url=prefiller_base_url)
+    app.state.decode_client = httpx.AsyncClient(timeout=None,
+                                                base_url=decoder_base_url)
+
+    yield
+
+    # Shutdown: Close clients
+    await app.state.prefill_client.aclose()
+    await app.state.decode_client.aclose()
+
+
+# Update FastAPI app initialization to use lifespan
+app = FastAPI(lifespan=lifespan)
+
+
+class StatsCalculator:
+
+    def __init__(self):
+        self._stats = []
+        self._last_log_time = time.time()
+
+    def add(self, value):
+        self._stats.append(value)
+        if time.time() - self._last_log_time > 5:
+            self._log_stats()
+            self._last_log_time = time.time()
+
+    def _log_stats(self):
+        # Print average, median, and 99th percentile
+        np_arr = np.array(self._stats)
+        output_str = (
+            f"\nNum requests: {len(self._stats)}" +
+            "\nPrefill node TTFT stats:" +
+            f"\n - Average (ms): {np.mean(np_arr)}" +
+            f"\n - Median (ms): {np.median(np_arr)}" +
+            f"\n - 99th Percentile (ms): {np.percentile(np_arr, 99)}\n")
+        print(
+            "===============================",
+            output_str,
+            "===============================",
+        )
+
+
+stats_calculator = StatsCalculator()
+counter = 0
+
+
+def parse_args():
+    parser = argparse.ArgumentParser()
+
+    parser.add_argument("--port", type=int, default=8000)
+    parser.add_argument("--host", type=str, default="localhost")
+    parser.add_argument("--prefiller-host", type=str, default="localhost")
+    parser.add_argument("--prefiller-port", type=int, default=8100)
+    parser.add_argument("--decoder-host", type=str, default="localhost")
+    parser.add_argument("--decoder-port", type=int, default=8200)
+    args = parser.parse_args()
+    return args
+
+
+# Initialize variables to hold the persistent clients
+app.state.prefill_client = None
+app.state.decode_client = None
+
+
+async def send_request_to_service(client: httpx.AsyncClient, endpoint: str,
+                                  req_data: dict, request_id: str):
+    """
+    Send a request to a service using a persistent client.
+    """
+    req_data = req_data.copy()
+    req_data["max_tokens"] = 1
+    if "max_completion_tokens" in req_data:
+        req_data["max_completion_tokens"] = 1
+
+    headers = {
+        "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
+        "X-Request-Id": request_id
+    }
+    response = await client.post(endpoint, json=req_data, headers=headers)
+    response.raise_for_status()
+    return response
+
+
+async def stream_service_response(client: httpx.AsyncClient, endpoint: str,
+                                  req_data: dict, request_id: str):
+    """
+    Asynchronously stream the response from a service using a persistent client.
+    """
+    headers = {
+        "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
+        "X-Request-Id": request_id
+    }
+    async with client.stream("POST", endpoint, json=req_data,
+                             headers=headers) as response:
+        response.raise_for_status()
+        async for chunk in response.aiter_bytes():
+            yield chunk
+
+
+@app.post("/v1/completions")
+async def handle_completions(request: Request):
+    global counter, stats_calculator
+    counter += 1
+
+    st = time.time()
+    try:
+        req_data = await request.json()
+        request_id = str(uuid4())
+
+        # Send request to prefill service, ignore the response
+        await send_request_to_service(app.state.prefill_client, "/completions",
+                                      req_data, request_id)
+
+        et = time.time()
+        stats_calculator.add(et - st)
+
+        # Stream response from decode service
+        async def generate_stream():
+            async for chunk in stream_service_response(app.state.decode_client,
+                                                       "/completions",
+                                                       req_data, request_id):
+                yield chunk
+
+        return StreamingResponse(generate_stream(),
+                                 media_type="text/event-stream")
+
+    except Exception as e:
+        import sys
+        import traceback
+
+        exc_info = sys.exc_info()
+        print(
+            "Error occurred in disagg prefill proxy server - completions endpoint"
+        )
+        print(e)
+        print("".join(traceback.format_exception(*exc_info)))
+        raise
+
+
+@app.post("/v1/chat/completions")
+async def handle_chat_completions(request: Request):
+    global counter, stats_calculator
+    counter += 1
+
+    st = time.time()
+    try:
+        req_data = await request.json()
+        request_id = str(uuid4())
+
+        # Send request to prefill service, ignore the response
+        await send_request_to_service(app.state.prefill_client,
+                                      "/chat/completions", req_data,
+                                      request_id)
+
+        et = time.time()
+        stats_calculator.add(et - st)
+
+        # Stream response from decode service
+        async def generate_stream():
+            async for chunk in stream_service_response(app.state.decode_client,
+                                                       "/chat/completions",
+                                                       req_data, request_id):
+                yield chunk
+
+        return StreamingResponse(generate_stream(),
+                                 media_type="text/event-stream")
+
+    except Exception as e:
+        import sys
+        import traceback
+
+        exc_info = sys.exc_info()
+        print(
+            "Error occurred in disagg prefill proxy server  - chat completions endpoint"
+        )
+        print(e)
+        print("".join(traceback.format_exception(*exc_info)))
+        raise
+
+
+if __name__ == "__main__":
+    global global_args
+    global_args = parse_args()
+
+    import uvicorn
+
+    uvicorn.run(app, host=global_args.host, port=global_args.port)
diff --git a/tests/e2e/pd_disaggreate/yuanrong/test_yuanrong_connector.py b/tests/e2e/pd_disaggreate/yuanrong/test_yuanrong_connector.py
new file mode 100644
index 0000000..e0c7d9d
--- /dev/null
+++ b/tests/e2e/pd_disaggreate/yuanrong/test_yuanrong_connector.py
@@ -0,0 +1,141 @@
+import json
+import os
+import signal
+import subprocess
+import time
+
+import pytest
+import requests
+
+HOST_IP = "127.0.0.1"
+MODEL_NAME = "Qwen/Qwen2.5-7B"
+WORKSPACE_DIR = "./tests/e2e/pd_disaggreate/yuanrong/"
+
+RUN_INSTANCES_SCRIPT = os.path.join(WORKSPACE_DIR,
+                                    "run_pd_instance.sh")
+RUN_PROXY_SERVER_SCRIPT = os.path.join(WORKSPACE_DIR, "run_proxy_server.sh")
+RUN_YUANRONG_SCRIPT = os.path.join(WORKSPACE_DIR, "run_yuanrong.sh")
+CLEAN_YUANRONG_SCRIPT = os.path.join(WORKSPACE_DIR, "clean_yuanrong.sh")
+PROXY_SERVER_SCRIPT = os.path.join(WORKSPACE_DIR, "simple_pd_proxy_server.py")
+PROXY_PORT = 8000
+PREFILL_PORT = 8100
+DECODE_PORT = 8200
+WORKER_PORT = 31530
+ETCD_PORT = 2411
+
+PROMPT_ANSWER = {
+    "who is the president of the united states?": "?\nDonald Trump"
+}
+RUN_INSTANCE_KEYWORDS = "vllm serve"
+RUN_PROXY_SERVER_KEYWORDS = "simple_pd_proxy_server.py"
+
+
+def start_yuanrong():
+    proc = subprocess.Popen([
+        "bash", RUN_YUANRONG_SCRIPT, f"{HOST_IP}", f"{WORKER_PORT}",
+        f"{ETCD_PORT}"
+    ])
+    proc.wait()
+
+
+def clean_yuanrong():
+    proc = subprocess.Popen(
+        ["bash", CLEAN_YUANRONG_SCRIPT, f"{HOST_IP}", f"{WORKER_PORT}"])
+    proc.wait()
+
+
+def start_instances():
+    proc = subprocess.Popen([
+        "bash", RUN_INSTANCES_SCRIPT, f"{MODEL_NAME}", f"{HOST_IP}",
+        f"{PREFILL_PORT}", f"{DECODE_PORT}"
+    ])
+    proc.wait()
+
+
+def start_proxy_server():
+    proc = subprocess.Popen([
+        "bash", RUN_PROXY_SERVER_SCRIPT, PROXY_SERVER_SCRIPT, f"{HOST_IP}",
+        f"{PROXY_PORT}", f"{PREFILL_PORT}", f"{DECODE_PORT}"
+    ])
+    proc.wait()
+
+
+def clean_instances_and_proxy_server():
+    instance_pids = get_pids_by_keyword(RUN_INSTANCE_KEYWORDS)
+    proxy_pids = get_pids_by_keyword(RUN_PROXY_SERVER_KEYWORDS)
+    for pid in proxy_pids + instance_pids:
+        pid = int(pid)
+        try:
+            os.kill(pid, signal.SIGINT)
+        except ProcessLookupError:
+            print(f"No such process with PID {pid}")
+        except PermissionError:
+            print(f"Permission denied to send SIGINT to PID {pid}")
+        except Exception as e:
+            print(f"Error: {e}")
+        time.sleep(3)
+        pid = int(pid)
+        try:
+            os.kill(pid, signal.SIGKILL)
+        except ProcessLookupError:
+            print(f"No such process with PID {pid}")
+        except PermissionError:
+            print(f"Permission denied to send SIGKILL to PID {pid}")
+        except Exception as e:
+            print(f"Error: {e}")
+
+
+def send_post_request(url, data):
+    try:
+        response = requests.post(url, json=data, timeout=10)
+        response.raise_for_status()
+        return response.text
+    except requests.exceptions.RequestException as e:
+        return f"Request failed: {e}"
+
+
+def get_pids_by_keyword(keyword):
+    try:
+        # Run 'ps aux' to get all running processes
+        result = subprocess.run(['ps', 'aux'],
+                                stdout=subprocess.PIPE,
+                                text=True)
+        lines = result.stdout.strip().split('\n')
+
+        matching_pids = []
+
+        for line in lines[1:]:  # Skip the header line
+            if keyword in line:
+                parts = line.split()
+                pid = parts[1]  # PID is the second column
+                matching_pids.append(pid)
+
+        return matching_pids
+    except Exception as e:
+        return f"error occurred trying to get PIDs of processes containing keyword {keyword}, error: {e}"
+
+
+@pytest.fixture
+def setup_and_clean_cluster():
+    start_yuanrong()
+    start_instances()
+    start_proxy_server()
+    time.sleep(3)
+    yield
+    clean_instances_and_proxy_server()
+    clean_yuanrong()
+
+
+def test_yuanrong_pd_dist(setup_and_clean_cluster):
+    proxy_url = f"http://{HOST_IP}:{PROXY_PORT}/v1/completions"
+    for prompt, answer in PROMPT_ANSWER.items():
+        data = {
+            "model": MODEL_NAME,
+            "prompt": prompt,
+            "max_tokens": 50,
+            "temperature": 0
+        }
+        response_str = send_post_request(proxy_url, data)
+        response_json = json.loads(response_str)
+        output = response_json["choices"][0]["text"]
+        assert output == answer, f"wrong response: {output}, expected: {answer}"
diff --git a/vllm_ascend/distributed/__init__.py b/vllm_ascend/distributed/__init__.py
index 0915b38..abd35ac 100644
--- a/vllm_ascend/distributed/__init__.py
+++ b/vllm_ascend/distributed/__init__.py
@@ -38,3 +38,8 @@ def register_connector():
         "MooncakeLayerwiseConnector",
         "vllm_ascend.distributed.mooncake_layerwise_connector",
         "MooncakeLayerwiseConnector")
+
+    KVConnectorFactory.register_connector(
+        "YuanRongConnector",
+        "vllm_ascend.distributed.yuanrong_connector",
+        "YuanRongConnector")
diff --git a/vllm_ascend/distributed/yuanrong_connector.py b/vllm_ascend/distributed/yuanrong_connector.py
new file mode 100644
index 0000000..6394233
--- /dev/null
+++ b/vllm_ascend/distributed/yuanrong_connector.py
@@ -0,0 +1,1326 @@
+# Copyright (c) Huawei Technologies Co., Ltd. 2025. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import asyncio
+import enum
+import hashlib
+import json
+import os
+import threading
+from collections import defaultdict
+from dataclasses import dataclass
+from types import SimpleNamespace
+from typing import (TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Set,
+                    Tuple, Union, cast)
+
+import numpy
+import torch
+from vllm.config import VllmConfig
+from vllm.distributed.kv_transfer.kv_connector.v1.base import (
+    KVConnectorBase_V1, KVConnectorMetadata, KVConnectorRole)
+from vllm.distributed.parallel_state import get_tp_group, get_world_group
+from vllm.multimodal.inputs import MultiModalFeatureSpec
+from vllm.utils import logger, split_host_port
+from vllm.v1.attention.backends.mla.common import MLACommonMetadata
+from vllm.v1.core.kv_cache_utils import _gen_mm_extra_hash_keys
+from vllm.v1.core.sched.output import SchedulerOutput
+
+if TYPE_CHECKING:
+    from vllm.attention.backends.abstract import AttentionMetadata
+    from vllm.forward_context import ForwardContext
+    from vllm.v1.request import Request
+
+from datasystem import DsTensorClient, Future
+
+# Configuration Constants
+DS_WORKER_ADDR = os.getenv("DS_WORKER_ADDR", "127.0.0.1:31501")
+ENABLE_PREFIX_CACHING = int(os.getenv("USING_PREFIX_CONNECTOR", "1"))
+FUTURE_TIMEOUT = int(os.getenv("FUTURE_TIMEOUT", "10000"))
+SYNC_FUTURE_TIMEOUT = int(os.getenv("SYNC_FUTURE_TIMEOUT", "1"))
+SLEEP_TIMEOUT = 0.005
+
+
+class RequestStatus(enum.IntEnum):
+    """Enumeration for tracking execution states of asynchronous KV operations.
+    
+    This enum defines the possible states of async save/load requests
+    throughout their lifecycle in the KV connector system.
+    """
+    WAITING = enum.auto()
+    TIMEOUT = enum.auto()
+    FINISHED = enum.auto()
+
+
+@dataclass
+class RequestTracker:
+    """Tracks request state for delayed KV cache saving and token scheduling.
+    
+    Maintains critical information about a request's token sequence, allocated
+    cache blocks, and scheduling progress to manage deferred save operations.
+    """
+    request_id: str
+    token_ids: torch.Tensor
+    block_ids: tuple[List[int], ...]
+    num_scheduled_tokens: int
+    mm_features: Optional[list[MultiModalFeatureSpec]] = None
+
+    @staticmethod
+    def from_new_request(
+        request_id: str,
+        token_ids: torch.Tensor,
+        block_ids: tuple[List[int], ...],
+        num_scheduled_tokens: int,
+        mm_features: Optional[list[MultiModalFeatureSpec]] = None
+    ) -> "RequestTracker":
+        """Creates a new RequestTracker instance for a fresh request.
+        
+        Args:
+            request_id: Unique identifier for the request
+            token_ids: Tensor containing the request's token sequence
+            block_ids: Tuple of allocated KV cache block ID lists
+            num_scheduled_tokens: Initial count of scheduled tokens
+            mm_features: Optional list of multimodal feature specifications
+        
+        Returns:
+            Initialized RequestTracker instance
+        """
+        return RequestTracker(request_id=request_id,
+                              token_ids=token_ids,
+                              block_ids=block_ids,
+                              num_scheduled_tokens=num_scheduled_tokens,
+                              mm_features=mm_features)
+
+    def update(self, block_ids: tuple[List[int], ...],
+               num_external_scheduled_tokens: int) -> None:
+        """Updates tracker state with newly allocated blocks and scheduled tokens.
+        
+        Args:
+            block_ids: Newly allocated block IDs (None if no new blocks)
+            num_external_scheduled_tokens: Number of additional tokens scheduled
+        """
+        if block_ids:
+            self.block_ids[0].extend(block_ids[0])
+
+        self.num_scheduled_tokens += num_external_scheduled_tokens
+
+
+@dataclass
+class ReqMeta:
+    """Metadata container for KV cache transfer operations (save/load).
+    
+    Encapsulates all necessary information for a single request's KV cache
+    transfer, including token data, block allocation, and transfer parameters.
+    """
+    request_id: str
+    token_ids: numpy.ndarray
+    block_ids: List[int]
+    request_rank: int
+    skip_block_num: int
+    ds_cached_block_num: int
+    need_save: bool
+    mm_features: Optional[list[MultiModalFeatureSpec]] = None
+
+    @staticmethod
+    def make_meta(
+        request_id: str,
+        token_ids: List[int],
+        block_ids: tuple[List[int], ...],
+        block_size: int,
+        request_rank: int,
+        skip_block_num: int,
+        ds_cached_block_num: int,
+        need_save: bool,
+        mm_features: Optional[list[MultiModalFeatureSpec]] = None
+    ) -> "ReqMeta":
+        """Factory method to create ReqMeta with block-aligned calculations.
+        
+        Calculates valid token count and block IDs aligned to the system's
+        block size to ensure proper KV cache alignment.
+        
+        Args:
+            request_id: Unique request identifier
+            token_ids: List of token IDs for the request
+            block_ids: Tuple of original block ID lists
+            block_size: Size of each KV cache block
+            request_rank: TP rank assigned to the request
+            skip_block_num: Number of blocks to skip
+            ds_cached_block_num: Number of blocks cached in datasystem
+            need_save: Whether the request needs to be saved
+            mm_features: Optional multimodal feature specifications
+        
+        Returns:
+            Initialized ReqMeta instance with aligned block data
+        """
+        # Calculate valid token count aligned to block size
+        valid_num_tokens = align_to_block_size(len(token_ids), block_size)
+        valid_block_ids_count = valid_num_tokens // block_size
+
+        return ReqMeta(request_id=request_id,
+                       token_ids=numpy.array(token_ids),
+                       block_ids=block_ids[0][:valid_block_ids_count],
+                       request_rank=request_rank,
+                       skip_block_num=skip_block_num,
+                       ds_cached_block_num=ds_cached_block_num,
+                       need_save=need_save,
+                       mm_features=mm_features)
+
+
+@dataclass
+class YuanRongConnectorMetadata(KVConnectorMetadata):
+    """Metadata container for YuanRong KV Connector operations.
+    
+    Extends the base KVConnectorMetadata to manage batches of requests with
+    round-robin TP rank assignment for load balancing across tensor parallel
+    workers.
+    """
+    requests: List[ReqMeta]
+
+    def __init__(self, tp_size: int, block_size: int):
+        """Initialize metadata container for YuanRong connector.
+        
+        Args:
+            tp_size: Tensor parallelism size (number of TP workers)
+            block_size: Size of each KV cache block
+        """
+        self._block_size = block_size
+        self.request_rank = 0  # Counter for round-robin rank assignment
+        self.requests = []
+        self._tp_size = tp_size
+
+    def add_request(
+            self,
+            request_id: str,
+            token_ids: List[int],
+            block_ids: tuple[List[int], ...],
+            skip_block_num: int,
+            ds_cached_block_num: int,
+            need_save: bool = True,
+            mm_features: Optional[list[MultiModalFeatureSpec]] = None) -> None:
+        """Adds request metadata to the batch with round-robin TP rank assignment.
+        
+        Assigns TP ranks in a round-robin fashion to distribute load evenly
+        across tensor parallel workers.
+        
+        Args:
+            request_id: Unique request identifier
+            token_ids: List of token IDs for the request
+            block_ids: Tuple of block ID lists
+            skip_block_num: Number of blocks to skip
+            ds_cached_block_num: Number of blocks cached in datasystem
+            need_save: Whether to save the request (default: True)
+            mm_features: Optional multimodal feature specifications
+        """
+        # Assign TP rank using round-robin distribution
+        request_rank = self.request_rank % self._tp_size
+        self.requests.append(
+            ReqMeta.make_meta(request_id=request_id,
+                              token_ids=token_ids,
+                              block_ids=block_ids,
+                              block_size=self._block_size,
+                              request_rank=request_rank,
+                              skip_block_num=skip_block_num,
+                              ds_cached_block_num=ds_cached_block_num,
+                              need_save=need_save,
+                              mm_features=mm_features))
+        self.request_rank = request_rank + 1
+
+
+@dataclass
+class ReqState:
+    """Tracks internal state of pending asynchronous save/load requests.
+    
+    Maintains counters for pending operations and completion status to
+    coordinate async KV transfer workflows.
+    """
+    num_pending: int = -1  # Number of pending async operations for the request
+    finished: bool = False  # Whether the request has been marked as finished
+
+
+class AsyncHandler:
+    """Manages asynchronous KV cache save/load operations.
+    
+    Coordinates background processing of async futures, tracks request states,
+    and handles completion/timeout events for KV transfer operations.
+    """
+
+    def __init__(self, role: KVConnectorRole, task_list: List[asyncio.Task]):
+        """Initialize async operation handler.
+        
+        Args:
+            role: KV connector role (PRODUCER/CONSUMER/WORKER)
+            task_list: List to register background async tasks
+        """
+        # Maps request IDs to their async state for save/load operations
+        self._async_save_reqs: Dict[str, ReqState] = defaultdict(ReqState)
+        self._async_load_reqs: Dict[str, ReqState] = defaultdict(ReqState)
+        self._is_producer: bool = role
+
+        # Queues for tracking completed operations
+        self._finished_save_reqs: asyncio.Queue = asyncio.Queue()
+        self._finished_load_reqs: asyncio.Queue = asyncio.Queue()
+
+        # Queues for pending futures
+        self._future_save_list: asyncio.Queue = asyncio.Queue()
+        self._future_load_list: asyncio.Queue = asyncio.Queue()
+
+        loop = asyncio.get_event_loop()
+
+        # Register background tasks based on role and configuration
+        if self._is_producer or ENABLE_PREFIX_CACHING:
+            task_list.append(loop.create_task(self.get_save_futures_async()))
+
+        if not self._is_producer or ENABLE_PREFIX_CACHING:
+            task_list.append(loop.create_task(self.get_load_futures_async()))
+
+    async def get_save_futures_async(self) -> None:
+        """Background task to monitor and process save operation futures.
+        
+        Continuously polls pending save futures, updates request states,
+        and handles completion/timeout events for asynchronous save operations.
+        """
+        while True:
+            try:
+                self._process_save_futures_batch()
+            except Exception as e:
+                logger.error("Failed to process save futures: %s", e)
+            finally:
+                await asyncio.sleep(SLEEP_TIMEOUT)
+
+    def _process_save_futures_batch(self) -> None:
+        """Handle a single batch of pending save futures."""
+        q_size = self._future_save_list.qsize()
+        for _ in range(q_size):
+            request_id, future = self._future_save_list.get_nowait()
+            self._handle_save_future(request_id, future)
+
+    def _handle_save_future(self, request_id: str, future: Future) -> None:
+        """Update save request state based on a future's result."""
+        req_state = self._async_save_reqs.get(request_id)
+        if req_state is None:
+            logger.warning("Req: %s, save state missing for future",
+                           request_id)
+            return
+
+        res = get_future(future)
+        if res == RequestStatus.FINISHED:
+            logger.info("Req: %s, Save task finished", request_id)
+            req_state.num_pending -= 1
+            if req_state.finished and req_state.num_pending == 0:
+                self._finished_save_reqs.put_nowait(request_id)
+                del self._async_save_reqs[request_id]
+            return
+
+        if res == RequestStatus.WAITING or not req_state.finished:
+            self._future_save_list.put_nowait((request_id, future))
+            return
+
+        logger.error("Request: %s save future timeout/failed, result: %s",
+                     request_id, res)
+        self._finished_save_reqs.put_nowait(request_id)
+        del self._async_save_reqs[request_id]
+
+    async def get_load_futures_async(self) -> None:
+        """Background task to monitor and process load operation futures.
+
+        Continuously polls pending load futures, updates request states,
+        and handles completion/timeout events for asynchronous load operations.
+        """
+        while True:
+            try:
+                self._process_load_futures_batch()
+            except Exception as e:
+                logger.error("Failed to process load futures: %s", e)
+            finally:
+                await asyncio.sleep(SLEEP_TIMEOUT)
+
+    def _process_load_futures_batch(self) -> None:
+        """Handle a single batch of pending load futures."""
+        q_size = self._future_load_list.qsize()
+        for _ in range(q_size):
+            request_id, future = self._future_load_list.get_nowait()
+            self._handle_load_future(request_id, future)
+
+    def _handle_load_future(self, request_id: str, future: Future) -> None:
+        """Update load request state based on a future's result."""
+        req_state = self._async_load_reqs.get(request_id)
+        if req_state is None:
+            logger.warning("Req: %s, load state missing for future",
+                           request_id)
+            return
+
+        res = get_future(future)
+        if res == RequestStatus.FINISHED:
+            logger.info("Req: %s, Load task finished", request_id)
+            req_state.num_pending -= 1
+            if req_state.num_pending == 0:
+                self._finished_load_reqs.put_nowait(request_id)
+                del self._async_load_reqs[request_id]
+            return
+
+        if res == RequestStatus.WAITING:
+            self._future_load_list.put_nowait((request_id, future))
+            return
+
+        logger.error("Req: %s, Load future timeout/failed, result: %s",
+                     request_id, res)
+        self._finished_load_reqs.put_nowait(request_id)
+        del self._async_load_reqs[request_id]
+
+    def add_save_request(self, request: ReqMeta, future_num: int) -> None:
+        """Register a save request with pending operation count.
+        
+        Args:
+            request: Request metadata
+            future_num: Number of async operations for this request
+        """
+        self._async_save_reqs[request.request_id].num_pending = future_num
+
+    def add_load_request(self, request: ReqMeta, future_num: int) -> None:
+        """Register a load request with pending operation count.
+        
+        Args:
+            request: Request metadata
+            future_num: Number of async operations for this request
+        """
+        self._async_load_reqs[request.request_id].num_pending = future_num
+
+    def add_save_future(self, request: ReqMeta, future: Future) -> None:
+        """Add a save operation future to the processing queue.
+        
+        Args:
+            request: Request metadata
+            future: Async future object for the save operation
+        """
+        self._future_save_list.put_nowait((request.request_id, future))
+
+    def add_load_future(self, request: ReqMeta, future: Future) -> None:
+        """Add a load operation future to the processing queue.
+        
+        Args:
+            request: Request metadata
+            future: Async future object for the load operation
+        """
+        self._future_load_list.put_nowait((request.request_id, future))
+
+    def get_save_finished(
+            self, finished_request_ids: Set[str]) -> Optional[Set[str]]:
+        """Retrieve IDs of requests with completed save operations.
+        
+        Marks requests as finished and checks for completed async operations,
+        returning IDs of fully completed save requests.
+        
+        Args:
+            finished_request_ids: Set of request IDs marked as finished
+        
+        Returns:
+            Set of completed save request IDs, or None if no completions
+        """
+        finished_reqs = set()
+        # Mark requests as finished and check completion status
+        for req_id in finished_request_ids:
+            req_state = self._async_save_reqs.get(req_id)
+            if req_state:
+                req_state.finished = True
+                if req_state.num_pending == 0:
+                    finished_reqs.add(req_id)
+                    del self._async_save_reqs[req_id]
+
+        # Retrieve completed requests from queue
+        while not self._finished_save_reqs.empty():
+            finished_reqs.add(self._finished_save_reqs.get_nowait())
+
+        if finished_reqs:
+            logger.debug("Finished save requests: %s, count: %d",
+                         finished_reqs, len(finished_reqs))
+            return finished_reqs
+        return None
+
+    def get_load_finished(self) -> Optional[Set[str]]:
+        """Retrieve IDs of requests with completed load operations.
+        
+        Returns IDs of requests with fully completed load operations.
+        
+        Returns:
+            Set of completed load request IDs, or None if no completions
+        """
+        finished_reqs = set()
+        while not self._finished_load_reqs.empty():
+            finished_reqs.add(self._finished_load_reqs.get_nowait())
+
+        if finished_reqs:
+            logger.debug("Finished load requests: %s, count: %d",
+                         finished_reqs, len(finished_reqs))
+            return finished_reqs
+        return None
+
+
+class YuanRongConnector(KVConnectorBase_V1):
+    """YuanRong datasystem KV cache connector implementation.
+    
+    Enables transfer of KV cache blocks between vLLM GPU memory and remote
+    YuanRong datasystem storage, supporting both synchronous and asynchronous
+    operations, prefix caching, and multimodal feature handling.
+    """
+
+    def __init__(self, vllm_config: "VllmConfig", role: KVConnectorRole):
+        """Initialize YuanRong KV connector.
+        
+        Args:
+            vllm_config: Core vLLM configuration object
+            role: KV connector role (WORKER/PRODUCER/CONSUMER)
+        """
+        super().__init__(vllm_config=vllm_config, role=role)
+
+        self.vllm_config = vllm_config
+        self._block_size = self.vllm_config.cache_config.block_size
+        self._is_producer = self.vllm_config.kv_transfer_config.is_kv_producer
+        self._tp_size = self.vllm_config.parallel_config.tensor_parallel_size
+        self._do_async_save = int(os.getenv("ASYNC_SAVE", 1))
+
+        # Mapping of requests needing load: request_id -> (Request object, block IDs)
+        self._requests_need_load: Dict[str, tuple[Request, tuple[list[int],
+                                                                 ...]]] = {}
+
+        # Model layer and cache management
+        self._layer_name_list: list[str] = []
+        self._kv_caches: list[torch.Tensor] = []
+        self._key_caches: list[torch.Tensor] = []
+        self._value_caches: list[torch.Tensor] = []
+
+        # Request state tracking
+        self._skip_blocks: Dict[str, int] = {}
+        self._ds_cached_blocks: Dict[str, int] = {}
+        self._delay_save: Dict[str, RequestTracker] = {}
+
+        # Async operation management
+        self._load_request_queue: asyncio.Queue[ReqMeta] = asyncio.Queue()
+        self._save_request_queue: asyncio.Queue[ReqMeta] = asyncio.Queue()
+        self._task_list: List[asyncio.Task] = []
+        self._async_handler = None
+
+        # Model backend flags
+        self._is_mla = False
+
+        # Datasystem connection configuration
+        ip, port = split_host_port(DS_WORKER_ADDR)
+        self._device = 0
+        self._tp_rank = None
+
+        if role == KVConnectorRole.WORKER:
+            # Initialize WORKER role components
+            self._tp_group = get_tp_group()
+            self._tp_rank = self._tp_group.rank_in_group
+            self._device = get_world_group().local_rank
+            self._ds_tensor_client = DsTensorClient(ip, port, self._device)
+            self._ds_tensor_client.init()
+
+            if self._do_async_save:
+                self._loop = asyncio.get_event_loop()
+                self._async_handler = AsyncHandler(self._is_producer,
+                                                   self._task_list)
+                # Register async processing tasks
+                if ENABLE_PREFIX_CACHING or not self._is_producer:
+                    self._task_list.append(
+                        self._loop.create_task(self.consumer_request_task()))
+
+                if ENABLE_PREFIX_CACHING or self._is_producer:
+                    self._task_list.append(
+                        self._loop.create_task(self.producer_request_task()))
+
+                # Start async event loop in daemon thread
+                thread = threading.Thread(target=self.start_event_loop,
+                                          daemon=True)
+                thread.start()
+        elif ENABLE_PREFIX_CACHING:
+            # Initialize datasystem client for non-WORKER roles with prefix caching
+            self._ds_tensor_client = DsTensorClient(ip, port, self._device)
+            self._ds_tensor_client.init()
+        else:
+            # Basic initialization for non-WORKER, non-caching roles
+            self._tp_group = None
+
+        logger.info(
+            "YuanRongConnector initialized successfully. "
+            "IP: %s, Port: %d, Device: %d", ip, port, self._device)
+
+    @staticmethod
+    def _to_block_tuple(block_ids: Any) -> tuple[list[int], ...]:
+        """Normalize block IDs to a tuple of integer lists."""
+        if block_ids is None:
+            return tuple()
+
+        if isinstance(block_ids, tuple):
+            return tuple([list(map(int, block)) for block in block_ids])
+
+        if isinstance(block_ids, list):
+            if block_ids and isinstance(block_ids[0], (list, tuple)):
+                return tuple([list(map(int, block)) for block in block_ids])
+            return (list(map(int, block_ids)), )
+
+        return (list(map(int, cast(Iterable[int], block_ids))), )
+
+    def start_event_loop(self):
+        """Start the async event loop in a dedicated thread.
+        
+        Runs the async task collection until all tasks complete, then closes
+        the event loop.
+        """
+        current_thread = threading.current_thread()
+        logger.info("Starting async event loop in thread: %s",
+                    current_thread.ident)
+        self._loop.run_until_complete(asyncio.gather(*self._task_list))
+        self._loop.close()
+
+    async def producer_request_task(self):
+        """Background task for processing save requests.
+        
+        Consumes the save request queue and executes KV cache save operations
+        for pending requests.
+        """
+        while True:
+            try:
+                q_size = self._save_request_queue.qsize()
+                for _ in range(q_size):
+                    request = self._save_request_queue.get_nowait()
+                    self.do_save_request(request)
+            except Exception as e:
+                logger.error("producer_request_task failed: %s", e)
+                # Re-queue request on failure (prevent loss)
+                self._save_request_queue.put_nowait(request)
+            finally:
+                await asyncio.sleep(SLEEP_TIMEOUT)
+
+    async def consumer_request_task(self):
+        """Background task for processing load requests.
+        
+        Consumes the load request queue and executes KV cache load operations
+        for pending requests.
+        """
+        while True:
+            try:
+                q_size = self._load_request_queue.qsize()
+                for _ in range(q_size):
+                    request = self._load_request_queue.get_nowait()
+                    self.do_load_kv(request)
+            except Exception as e:
+                logger.error("consumer_request_task failed: %s", e)
+                # Re-queue request on failure (prevent loss)
+                self._load_request_queue.put_nowait(request)
+            finally:
+                await asyncio.sleep(SLEEP_TIMEOUT)
+
+    def generate_kv_cache_token_key(self, request: ReqMeta,
+                                    block_start_index: int,
+                                    block_end_index: int) -> List[str]:
+        """Generate unique SHA256 keys for KV cache blocks.
+        
+        Creates unique identifiers for KV cache blocks based on token content,
+        block indices, TP rank, and multimodal features to ensure cache
+        consistency and uniqueness.
+        
+        Args:
+            request: Request metadata
+            block_start_index: Starting block index for key generation
+            block_end_index: Ending block index for key generation
+        
+        Returns:
+            List of SHA256 hash keys for the specified blocks
+        """
+        # Use TP rank for non-MLA architectures, fixed 0 for MLA
+        if not self._is_mla:
+            external_key = "-" + str(self._tp_rank)
+        else:
+            external_key = "-0"
+
+        return generate_hash_sha256(block_start_index, block_end_index,
+                                    request.token_ids, self._block_size,
+                                    external_key, request.mm_features)
+
+    def start_load_kv(self, forward_context: "ForwardContext",
+                      **kwargs) -> None:
+        """Initiate KV cache loading process.
+        
+        Triggers loading of KV cache blocks from datasystem to GPU memory for
+        eligible requests.
+        
+        Args:
+            forward_context: Forward pass context object
+            **kwargs: Additional keyword arguments
+        """
+        # Skip loading for producers with prefix caching disabled
+        if self._is_producer and not ENABLE_PREFIX_CACHING:
+            return
+
+        # Retrieve connector metadata
+        metadata: KVConnectorMetadata = self._get_connector_metadata()
+        if not metadata.requests:
+            return
+
+        # Initialize KV cache references if not already done
+        if not self._kv_caches:
+            self._init_kv_caches_from_forward_context(forward_context)
+
+        # Distribute load requests to processing queue or direct execution
+        for request in metadata.requests:
+            if self._async_handler is not None:
+                self._load_request_queue.put_nowait(request)
+            else:
+                self.do_load_kv(request)
+
+    def get_finished(
+        self, finished_req_ids: Set[str]
+    ) -> Tuple[Optional[Set[str]], Optional[Set[str]]]:
+        """Retrieve IDs of requests with completed save/load operations.
+        
+        Gets sets of request IDs that have finished save and/or load operations
+        from the async handler.
+        
+        Args:
+            finished_req_ids: Set of request IDs marked as finished
+        
+        Returns:
+            Tuple containing (completed save requests, completed load requests),
+            with None for empty sets
+        """
+        finished_saved_req, finished_loaded_req = None, None
+        if self._async_handler is not None:
+            # Get completed save requests
+            if self._is_producer or ENABLE_PREFIX_CACHING:
+                finished_saved_req = self._async_handler.get_save_finished(
+                    finished_req_ids)
+
+            # Get completed load requests
+            if not self._is_producer or ENABLE_PREFIX_CACHING:
+                finished_loaded_req = self._async_handler.get_load_finished()
+
+            logger.debug(
+                "Finished saved requests: %s, Finished loaded requests: %s",
+                finished_saved_req, finished_loaded_req)
+            return finished_saved_req, finished_loaded_req
+        return None, None
+
+    def get_sending_count(self) -> int:
+        """Get expected number of send operations based on model architecture.
+        
+        Returns the number of send operations required (1 for MLA architecture,
+        TP size for non-MLA architecture).
+        
+        Returns:
+            Number of expected send operations
+        """
+        if self._is_mla:
+            return 1
+        return self._tp_size
+
+    def do_load_kv(self, request: ReqMeta) -> None:
+        """Execute KV cache load operation (Host to Device).
+        
+        Loads KV cache blocks from datasystem to GPU memory, supporting both
+        MLA (unified KV) and non-MLA (split Key/Value) architectures.
+        
+        Args:
+            request: Request metadata for the load operation
+        """
+        ds_cached_block_num = request.ds_cached_block_num
+        skip_block_num = request.skip_block_num
+        logger.debug("Req: %s, ds_cached_blocks: %d, skip_blocks: %d",
+                     request.request_id, ds_cached_block_num, skip_block_num)
+
+        # Skip if no cached blocks available
+        if ds_cached_block_num == 0:
+            return
+
+        # Generate cache keys for the blocks to load
+        key_list = self.generate_kv_cache_token_key(request, skip_block_num,
+                                                    ds_cached_block_num)
+        block_id_list = request.block_ids
+
+        if not block_id_list or not key_list:
+            return
+
+        # Handle non-MLA architecture (split Key/Value cache)
+        if not self._is_mla:
+            if len(key_list) != len(block_id_list):
+                logger.error(
+                    "Req: %s, Mismatch: key_list(len=%d) vs block_id_list(len=%d)",
+                    request.request_id,
+                    len(key_list),
+                    len(block_id_list),
+                )
+
+            key_load_future = self._ds_tensor_client.mget_page_attn_blockwise_h2d(
+                key_list, self._key_caches, block_id_list, FUTURE_TIMEOUT)
+            value_cache_key_list = [key + "-value" for key in key_list]
+            value_load_future = self._ds_tensor_client.mget_page_attn_blockwise_h2d(
+                value_cache_key_list, self._value_caches, block_id_list,
+                FUTURE_TIMEOUT)
+
+            # Handle synchronous/asynchronous execution
+            if not self._do_async_save:
+                get_future(key_load_future, SYNC_FUTURE_TIMEOUT)
+                get_future(value_load_future, SYNC_FUTURE_TIMEOUT)
+            elif self._async_handler is not None:
+                self._async_handler.add_load_request(request, 2)
+                self._async_handler.add_load_future(request, key_load_future)
+                self._async_handler.add_load_future(request, value_load_future)
+
+            logger.debug("Successfully mget_h2d (Split KV) for %s",
+                         request.request_id)
+            return
+
+        # Handle MLA architecture (unified KV cache)
+        future = self._ds_tensor_client.mget_page_attn_blockwise_h2d(
+            key_list, self._kv_caches, block_id_list)
+        if not self._do_async_save:
+            get_future(future, SYNC_FUTURE_TIMEOUT)
+        elif self._async_handler is not None:
+            self._async_handler.add_load_request(request, 1)
+            self._async_handler.add_load_future(request, future)
+
+        logger.debug("Successfully mget_h2d (MLA) for %s", request.request_id)
+
+    def wait_for_layer_load(self, layer_name: str) -> None:
+        """YuanRongConnector does not do layerwise saving."""
+        return
+
+    def save_kv_layer(self, layer_name: str, kv_layer: torch.Tensor,
+                      attn_metadata: "AttentionMetadata", **kwargs) -> None:
+        """Register KV cache layer for transfer operations.
+        
+        Registers model layer KV cache references with the connector,
+        distinguishing between MLA and non-MLA architectures.
+        
+        Args:
+            layer_name: Name of the model layer
+            kv_layer: KV cache tensor(s) (single tensor for MLA, tuple for non-MLA)
+            attn_metadata: Attention metadata for the layer
+            **kwargs: Additional keyword arguments
+        """
+        # Skip for non-producers with prefix caching disabled
+        if not ENABLE_PREFIX_CACHING and not self._is_producer:
+            return
+
+        # Register new layers
+        if layer_name not in self._layer_name_list:
+            self._layer_name_list.append(layer_name)
+            self._is_mla = isinstance(attn_metadata, MLACommonMetadata)
+
+            # Register cache references based on architecture
+            if self._is_mla:
+                self._kv_caches.append(kv_layer)
+            else:
+                self._key_caches.append(kv_layer[0])
+                self._value_caches.append(kv_layer[1])
+
+    def _handle_save_futures(self, request: ReqMeta,
+                             futures: list[Any]) -> None:
+        """Handle synchronous or asynchronous save futures.
+
+        Args:
+            request: Request metadata object.
+            futures: Futures returned by the datasystem client save calls.
+        """
+        if not self._do_async_save:
+            for future in futures:
+                get_future(future, SYNC_FUTURE_TIMEOUT)
+        elif self._async_handler is not None:
+            self._async_handler.add_save_request(request, len(futures))
+            for future in futures:
+                self._async_handler.add_save_future(request, future)
+
+    def do_save_request(self, request: ReqMeta) -> None:
+        """Execute KV cache save operation (Device to Host).
+        
+        Saves KV cache blocks from GPU memory to datasystem, supporting both
+        MLA (unified KV) and non-MLA (split Key/Value) architectures.
+        
+        Args:
+            request: Request metadata for the save operation
+        """
+        logger.debug("Req: %s, Save request", request)
+        # Skip for non-producers or requests not marked for save
+        if not self._is_producer or not request.need_save:
+            return
+
+        # For MLA architecture, only the assigned TP rank performs the save
+        if self._is_mla and self._tp_rank != request.request_rank:
+            return
+
+        # Skip if no blocks to save
+        if not request.block_ids:
+            return
+
+        token_key_list = self.generate_kv_cache_token_key(
+            request, 0, len(request.block_ids))
+
+        save_futures: list[Any] = []
+
+        # Handle non-MLA architecture (split Key/Value cache)
+        if not self._is_mla:
+            key_save_future = self._ds_tensor_client.mset_page_attn_blockwise_d2h(
+                token_key_list, self._key_caches, request.block_ids)
+            value_cache_key_list = [key + "-value" for key in token_key_list]
+            value_save_future = self._ds_tensor_client.mset_page_attn_blockwise_d2h(
+                value_cache_key_list, self._value_caches, request.block_ids)
+
+            save_futures.extend([key_save_future, value_save_future])
+
+            self._handle_save_futures(request, save_futures)
+            logger.debug("Successfully mset_d2h (Split KV) for %s",
+                         request.request_id)
+            return
+
+        # Handle MLA architecture (unified KV cache)
+        future = self._ds_tensor_client.mset_page_attn_blockwise_d2h(
+            token_key_list, self._kv_caches, request.block_ids)
+        self._handle_save_futures(request, [future])
+
+        logger.debug("Successfully mset_d2h (MLA) for %s", request.request_id)
+
+    def wait_for_save(self) -> None:
+        """Trigger save process for pending requests.
+        
+        Initiates save operations for requests in the metadata batch, either
+        through async queue or direct execution.
+        """
+        # Skip for non-producer roles
+        if not self._is_producer:
+            return
+
+        connector_metadata = self._get_connector_metadata()
+        if not isinstance(connector_metadata, YuanRongConnectorMetadata):
+            raise ValueError(
+                "The connector_metadata must be instance of YuanRongConnectorMetadata"
+            )
+
+        # Skip if no requests to save
+        if not connector_metadata.requests:
+            return
+
+        for request in connector_metadata.requests:
+            if self._async_handler is not None:
+                self._save_request_queue.put_nowait(request)
+            else:
+                self.do_save_request(request)
+
+    def get_num_new_matched_tokens(
+        self,
+        request: "Request",
+        num_computed_tokens: int,
+    ) -> Tuple[int, bool]:
+        """Calculate number of tokens retrievable from external cache.
+        
+        Determines how many tokens can be loaded from the datasystem cache
+        instead of being computed, with separate logic for producer and
+        consumer roles.
+        
+        Args:
+            request: Request object
+            num_computed_tokens: Number of tokens already computed
+        
+        Returns:
+            Tuple containing (number of external tokens, async load flag)
+        """
+        num_computed_blocks = num_computed_tokens // self._block_size
+        num_tokens_to_check = align_to_block_size(
+            len(request.prompt_token_ids), self._block_size)
+        prompt_blocks = num_tokens_to_check // self._block_size
+        num_external_hit_tokens = 0
+
+        # Consumer/Non-Producer role logic
+        if not self._is_producer:
+            self._skip_blocks[request.request_id] = num_computed_blocks
+            num_external_computed_tokens = len(
+                request.prompt_token_ids) - num_computed_tokens - 1
+            self._ds_cached_blocks[request.request_id] = prompt_blocks
+
+            if self._do_async_save and num_external_computed_tokens > 0:
+                logger.info(
+                    "Req: %s, Computed tokens: %d, External computed tokens: %d",
+                    request.request_id, num_computed_tokens,
+                    num_external_computed_tokens)
+                return num_external_computed_tokens, True
+
+            return num_external_computed_tokens, False
+
+        # Producer role with prefix caching logic
+        if ENABLE_PREFIX_CACHING:
+            tokens = request.prompt_token_ids
+            mm_features = self._get_mm_features(request)
+            keys = generate_hash_sha256(num_computed_blocks, prompt_blocks,
+                                        numpy.array(tokens), self._block_size,
+                                        "-0", mm_features)
+
+            if not keys:
+                logger.info(
+                    "Req: %s, Total tokens %d, HBM hit tokens: %d, "
+                    "External hit tokens: 0", request.request_id,
+                    request.num_tokens, num_computed_tokens)
+                return 0, False
+
+            try:
+                exists = self._ds_tensor_client.exist(keys) + [False]
+            except RuntimeError:
+                logger.info(
+                    "Req: %s, Total tokens %d, HBM hit tokens: %d, "
+                    "External hit tokens: 0", request.request_id,
+                    request.num_tokens, num_computed_tokens)
+                return 0, False
+
+            num_external_hit_blocks = exists.index(False)
+            num_external_hit_tokens = num_external_hit_blocks * self._block_size
+
+            self._skip_blocks[request.request_id] = num_computed_blocks
+            self._ds_cached_blocks[
+                request.
+                request_id] = num_external_hit_blocks + num_computed_blocks
+
+            logger.info(
+                "Req: %s, Total tokens %d, HBM hit tokens: %d, "
+                "External hit tokens: %d", request.request_id,
+                request.num_tokens, num_computed_tokens,
+                num_external_hit_tokens)
+
+            if self._do_async_save and num_external_hit_tokens > 0:
+                return num_external_hit_tokens, True
+
+        return num_external_hit_tokens, False
+
+    def update_state_after_alloc(self, request: "Request", blocks: Any,
+                                 num_external_tokens: int) -> None:
+        """Update internal state after block allocation by scheduler.
+        
+        Records newly allocated blocks for requests that need external cache
+        loading, updating the internal request tracking state.
+        
+        Args:
+            request: Request object
+            blocks: Allocated block information
+            num_external_tokens: Number of external tokens to load
+        """
+        if num_external_tokens > 0:
+            block_ids: tuple[list[int], ...] = self._to_block_tuple(
+                blocks.get_unhashed_block_ids())
+            self._requests_need_load[request.request_id] = (request, block_ids)
+            logger.debug("Req: %s, Added to load queue", request.request_id)
+
+    def build_connector_meta(
+        self,
+        scheduler_output: SchedulerOutput,
+    ) -> KVConnectorMetadata:
+        """Construct metadata for KV transfer operations.
+        
+        Builds a metadata batch containing all necessary information for
+        KV cache save/load operations based on scheduler output.
+        
+        Args:
+            scheduler_output: Output from the vLLM scheduler
+        
+        Returns:
+            Constructed YuanRongConnectorMetadata instance
+        
+        Raises:
+            ValueError: If there's a mismatch in load request tracking
+        """
+        meta = YuanRongConnectorMetadata(self._tp_size, self._block_size)
+        total_need_load = 0
+        total_need_load += self._handle_new_requests(scheduler_output, meta)
+        total_need_load += self._handle_cached_requests(scheduler_output, meta)
+        total_need_load += self._handle_pending_async_loads(meta)
+        logger.debug("Build Meta: total_need_load=%s, pending=%s",
+                     total_need_load, len(self._requests_need_load))
+        if total_need_load != len(self._requests_need_load):
+            logger.error("Mismatch: need_load=%s vs pending=%s",
+                         total_need_load, len(self._requests_need_load))
+            raise ValueError("Internal state mismatch in load requests")
+
+        self._requests_need_load.clear()
+        return meta
+
+    def _handle_new_requests(self, scheduler_output: SchedulerOutput,
+                             meta: YuanRongConnectorMetadata) -> int:
+        """Handle metadata construction for newly scheduled requests."""
+        total_need_load = 0
+        for new_req in scheduler_output.scheduled_new_reqs:
+            mm_features = self._get_mm_features(new_req)
+            block_ids = self._to_block_tuple(new_req.block_ids)
+
+            if new_req.req_id in self._requests_need_load:
+                self._add_request_to_meta(meta, new_req.req_id,
+                                          new_req.prompt_token_ids, block_ids,
+                                          mm_features)
+                total_need_load += 1
+                continue
+
+            if not self._is_producer:
+                continue
+
+            num_scheduled_tokens = scheduler_output.num_scheduled_tokens.get(
+                new_req.req_id, 0)
+            num_scheduled_tokens += new_req.num_computed_tokens
+
+            # Track for delayed save if not all tokens are scheduled
+            if len(new_req.prompt_token_ids) > num_scheduled_tokens:
+                self._delay_save[
+                    new_req.req_id] = RequestTracker.from_new_request(
+                        new_req.req_id, new_req.prompt_token_ids, block_ids,
+                        num_scheduled_tokens, mm_features)
+                continue
+
+            self._add_request_to_meta(meta, new_req.req_id,
+                                      new_req.prompt_token_ids, block_ids,
+                                      mm_features)
+
+        return total_need_load
+
+    def _handle_cached_requests(self, scheduler_output: SchedulerOutput,
+                                meta: YuanRongConnectorMetadata) -> int:
+        """Handle metadata construction for cached/suspended requests."""
+        total_need_load = 0
+        cached_reqs = scheduler_output.scheduled_cached_reqs
+        for i, req_id in enumerate(cached_reqs.req_ids):
+            new_block_ids = cached_reqs.new_block_ids[i]
+            resumed_from_preemption = cached_reqs.resumed_from_preemption[i]
+
+            # Handle delayed save requests
+            if not resumed_from_preemption and req_id in self._delay_save:
+                request_tracker = self._delay_save.get(req_id)
+                num_external_scheduled_tokens = scheduler_output.num_scheduled_tokens.get(
+                    req_id, 0)
+                if request_tracker is not None:
+                    request_tracker.update(self._to_block_tuple(new_block_ids),
+                                           num_external_scheduled_tokens)
+                    # Move to save queue if all tokens are scheduled
+                    if len(request_tracker.token_ids
+                           ) <= request_tracker.num_scheduled_tokens:
+                        del self._delay_save[req_id]
+                        logger.debug(
+                            "Req: %s, Processing load for delayed save request",
+                            request_tracker.request_id)
+                        self._add_request_to_meta(meta,
+                                                  request_tracker.request_id,
+                                                  request_tracker.token_ids,
+                                                  request_tracker.block_ids,
+                                                  request_tracker.mm_features)
+
+            # Handle resumed requests needing load
+            if req_id in self._requests_need_load:
+                request, _ = self._requests_need_load[req_id]
+                mm_features = self._get_mm_features(request)
+                logger.debug("Req: %s, Processing load for resumed request",
+                             req_id)
+                self._add_request_to_meta(meta, req_id,
+                                          request.prompt_token_ids,
+                                          self._to_block_tuple(new_block_ids),
+                                          mm_features)
+                total_need_load += 1
+
+        return total_need_load
+
+    def _handle_pending_async_loads(self,
+                                    meta: YuanRongConnectorMetadata) -> int:
+        """Handle pending async load requests when async save is enabled."""
+        total_need_load = 0
+        if not self._do_async_save:
+            return total_need_load
+
+        for req_id, (req, block_ids) in self._requests_need_load.items():
+            if not block_ids:
+                logger.debug("Req: %s, Skipping empty block load request",
+                             req_id)
+                continue
+
+            self._add_request_to_meta(meta,
+                                      req_id,
+                                      req.prompt_token_ids,
+                                      block_ids,
+                                      self._get_mm_features(req),
+                                      need_save=False)
+            total_need_load += 1
+
+        return total_need_load
+
+    def _add_request_to_meta(
+        self,
+        meta: YuanRongConnectorMetadata,
+        request_id: str,
+        token_ids: Union[Iterable[int], numpy.ndarray[Any, Any]],
+        block_ids: tuple[List[int], ...],
+        mm_features: Optional[list[MultiModalFeatureSpec]] = None,
+        need_save: bool = True,
+    ) -> None:
+        """Common helper to add a request to metadata with tracking pops."""
+        skip_block_num = self._skip_blocks.pop(request_id, 0)
+        ds_cached_block_num = self._ds_cached_blocks.pop(request_id, 0)
+        token_id_list = [int(token_id) for token_id in token_ids]
+        meta.add_request(
+            request_id=request_id,
+            token_ids=token_id_list,
+            block_ids=block_ids,
+            skip_block_num=skip_block_num,
+            ds_cached_block_num=ds_cached_block_num,
+            need_save=need_save,
+            mm_features=mm_features,
+        )
+
+    @staticmethod
+    def _get_mm_features(
+            request: Any) -> Optional[list[MultiModalFeatureSpec]]:
+        """Extract multimodal features from request-like objects."""
+        return request.mm_features if hasattr(request, "mm_features") else None
+
+    def request_finished(
+        self,
+        request: "Request",
+        block_ids: List[int],
+    ) -> Tuple[bool, Optional[Dict[str, Any]]]:
+        """Callback for completed requests.
+        
+        Notifies the connector that a request has finished processing and
+        indicates if asynchronous saving is still in progress.
+        
+        Args:
+            request: Completed request object
+            block_ids: List of block IDs associated with the request
+        
+        Returns:
+            Tuple containing (async save in progress flag, additional metadata)
+        """
+        logger.debug("Request finished: request=%s, block_ids=%s", request,
+                     block_ids)
+        # Return async save status for producer roles
+        if self._is_producer:
+            return bool(self._do_async_save), None
+        return False, None
+
+    def _init_kv_caches_from_forward_context(
+            self, forward_context: "ForwardContext") -> None:
+        """Initialize KV cache references from forward context.
+        
+        Extracts KV cache references from the vLLM forward context and
+        initializes the connector's cache tracking structures, detecting
+        the model architecture type.
+        
+        Args:
+            forward_context: vLLM forward pass context object
+        """
+        attn_metadata = forward_context.attn_metadata
+        no_compile_layers = forward_context.no_compile_layers
+        for layer_name, attn_layer in no_compile_layers.items():
+            if not hasattr(attn_layer, 'kv_cache'):
+                continue
+
+            kv_layer = attn_layer.kv_cache[forward_context.virtual_engine]
+            self._is_mla = isinstance(attn_metadata, MLACommonMetadata)
+            if layer_name not in self._layer_name_list:
+                self._layer_name_list.append(layer_name)
+                logger.debug("Init cache for layer: %s", layer_name)
+                # Register cache tensors based on architecture
+                if not self._is_mla:
+                    self._key_caches.append(kv_layer[0])
+                    self._value_caches.append(kv_layer[1])
+                else:
+                    self._kv_caches.append(kv_layer)
+
+
+# Utility Functions
+def align_to_block_size(num_tokens: int, block_size: int) -> int:
+    """Align token count to nearest block size boundary.
+    
+    Uses ceiling division to round up the token count to the next
+    block size multiple, ensuring proper cache block alignment.
+    
+    Args:
+        num_tokens: Number of tokens to align
+        block_size: Size of each cache block
+        
+    Returns:
+        Token count aligned to block size
+    """
+    return (num_tokens + block_size - 2) // block_size * block_size
+
+
+def generate_hash_sha256(
+        block_start_index: int,
+        block_end_index: int,
+        token_ids: numpy.ndarray,
+        block_size: int,
+        external_key: str,
+        mm_features: Optional[list[MultiModalFeatureSpec]] = None
+) -> List[str]:
+    """Generate SHA256 hash keys for KV cache blocks.
+    
+    Creates unique hash identifiers for each cache block based on token
+    content, block indices, external keys, and multimodal features.
+    
+    Args:
+        block_start_index: Starting block index
+        block_end_index: Ending block index
+        token_ids: Array of token IDs
+        block_size: Size of each cache block
+        external_key: External identifier (e.g., TP rank)
+        mm_features: Optional multimodal feature specifications
+        
+    Returns:
+        List of SHA256 hash keys for the specified blocks
+    """
+    hash_list = []
+    for block_index in range(block_start_index, block_end_index):
+        end_index = (block_index + 1) * block_size
+        input_ids = token_ids[:end_index]
+        input_ids_bytes = input_ids.tobytes()
+
+        extra_bytes = b""
+        if mm_features:
+            start_token_idx = block_index * block_size
+            end_token_idx = end_index
+            mm_request = SimpleNamespace(mm_features=mm_features)
+            mm_extra_keys, _ = _gen_mm_extra_hash_keys(mm_request,
+                                                       start_token_idx,
+                                                       end_token_idx, 0)
+            if mm_extra_keys:
+                extra_bytes = json.dumps(mm_extra_keys,
+                                         separators=(',', ':')).encode("utf-8")
+
+        # Combine all components and generate hash
+        combined_bytes = input_ids_bytes + extra_bytes + external_key.encode(
+            "utf-8")
+        token_hash = hashlib.sha256(combined_bytes).hexdigest()
+        hash_list.append(token_hash)
+    return hash_list
+
+
+def get_future(fut: Future, timeout: int = FUTURE_TIMEOUT) -> RequestStatus:
+    """Resolve future with timeout handling.
+    
+    Waits for a future to complete with specified timeout, returning
+    the appropriate status based on completion, timeout, or failure.
+    
+    Args:
+        fut: Future object to resolve
+        timeout: Timeout in milliseconds (default: FUTURE_TIMEOUT)
+        
+    Returns:
+        RequestStatus indicating future outcome
+    """
+    try:
+        failed_list = fut.get(timeout)
+    except TimeoutError:
+        return RequestStatus.WAITING
+
+    if len(failed_list) != 0:
+        logger.error("Future returned failures: %s" % failed_list)
+        return RequestStatus.TIMEOUT
+
+    return RequestStatus.FINISHED
-- 
2.43.0

